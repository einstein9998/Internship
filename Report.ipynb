{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an investigation of various Approximate Nearest Neighbors (ANN) algorithms for use in machine learning.  The main core of each test is as follows:  \n",
    "\n",
    "* A query is sampled from the set of compounds that target EGFR and are deemed to be relatively active (the query is 20% of this set)\n",
    "* An ANN model is created and trained on the remaining EGFR compounds (both active and inactive) and a database of 200000 other compounds\n",
    "* The query is run on the model, and the quality of the test is determined by the proportion of neighbors recieved that are members of the original active EGFR compounds\n",
    "\n",
    "The variations between tests come from the type of model and the hyperparameters for each model.  Each combination is also repeated for five trials, using a different random seed for the query sampling.  The different models are from the Python package scikit-learn, Facebook's Faiss algorithms, and Spotify's Annoy algorithms.  There is also an analysis utilizing scikit-learn's clustering algorithms with Faiss to offer an additional perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    " 1. [sklearn](sklearn.ipynb)\n",
    " 2. [faiss](Faiss.ipynb)\n",
    " 3. [annoy](Annoy.ipynb)\n",
    " 4. [comparing all 3](all3.ipynb)\n",
    " 5. [clustering](Clustering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All algorithms return a similar quality. Query times for all algorithms except ball_tree are reasonable (around 5 seconds), but not super fast.  Ball_tree is longer (around 30 seconds).  Within the faster algorithms, kd_tree is fastest, followed by brute-force with euclidean distance metric, followed by brute-force with cosine distance metric.\n",
    "* The tree algorithms take longer (around 15 seconds) than the brute-force (around 0.1 seconds) to build, which makes sense as trees are being created, as opposed to the very little setup required for the brute-force model.  Note that build time is largely inconsequential, as an application of this would most likely include build a model once, and then running many queries on the same model.\n",
    "* From the information gathered, it appears as if the kd_tree algorithm is the best of the four, although not by very much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All algorithms return a similar quality.  Query times vary from 0.01 seconds for HNSWFlat to around 0.7 seconds for FlatL2 and FlatIP. The times are overall faster than those from sklearn while maintaining a similar level of quality.\n",
    "* The build times are about 1 second with the exception of HNSWFlat, which takes about 44 seconds.  This extra time seems worth the extremely fast query time available from this algorithm.  \n",
    "* From the information gathered, it appears as if the HNSWFlat algorithm is the best of the five, although not by very much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All algorithms return a similar quality. The hamming distance metric seems to have a slightly lower quality, as well as more variability in query time and build time. \n",
    "* The build times for the annoy algorithms are significantly greater than those of other algorithms.  This is because Annoy does not support model-building with an entire dataframe.  Rather, each vector must be added individually to the model.  Again, this is not likely to be impactful, but is worth considering.\n",
    "* From the information gathered, it appears as if the hamming distance metric is the worth of the four.  The other 3 are all good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can split the algorithms into three different classes based on build time.  Call class 1 those with a mean build time less than 5 seconds, class 2 those with a mean build time between 5 and 100 seconds, and class 3 those with a mean build time greater than 100 seconds.\n",
    "    * Class 1:\n",
    "        * sklearn/brute/cosine\n",
    "        * sklearn/brute/euclidean\n",
    "        * faiss/FlatL2\n",
    "        * faiss/FlatIP\n",
    "        * faiss/LSH\n",
    "        * faiss/PQ\n",
    "    * Class 2:\n",
    "        * sklearn/ball_tree/euclidean\n",
    "        * sklearn/kd_tree/euclidean\n",
    "        * faiss/HNSWFlat\n",
    "    * Class 3:\n",
    "        * annoy/angular\n",
    "        * annoy/euclidean\n",
    "        * annoy/hamming\n",
    "        * annoy/manhattan\n",
    "        \n",
    "* We can also do the same for query time.  Call class 1 those with a mean query time less than 1 second, class 2 those with a mean query time between 1 and 10 seconds, and class 3 those with a mean query time greater than 10 seconds.\n",
    "    * Class 1:\n",
    "        * faiss/FlatL2\n",
    "        * faiss/FlatIP\n",
    "        * faiss/HNSWFlat\n",
    "        * faiss/LSH\n",
    "        * faiss/PQ\n",
    "    * Class 2:\n",
    "        * sklearn/brute/cosine\n",
    "        * sklearn/brute/euclidean\n",
    "        * sklearn/kd_tree/euclidean\n",
    "        * annoy/angular\n",
    "        * annoy/euclidean\n",
    "        * annoy/hamming\n",
    "        * annoy/manhattan\n",
    "    * Class 3:\n",
    "        * sklearn/ball_tree_euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Build Time | Class 1 (t<5)                              | Class 2 (5<t<100)           | Class 3 (t>100)                                             |\n",
    "|------------|------------|:------------------------------------------:|:---------------------------:|:----------------------------------------------------------:|\n",
    "| Query Time |            |                                              |                             |                                                             |\n",
    "| Class 1 (t<1)|            | faiss/FlatL2 <br> faiss/FlatIP <br> faiss/LSH <br> faiss/PQ | faiss/HNSWFlat              |                                                             |\n",
    "| Class 2 (1<t<10)|            | sklearn/brute/cosine <br> sklearn/brute/euclidean | sklearn/kd_tree/euclidean   | annoy/angular <br> annoy/euclidean <br> annoy/hamming <br> annoy/manhattan |\n",
    "| Class 3 (t>10)|            |                                              | sklearn/ball_tree/euclidean |                                                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower classes are better, so we can create a rough hierarchy: faiss (except HNSWFlat), then skearn brute and faiss HNSWFlat, then sklearn kd_tree, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
